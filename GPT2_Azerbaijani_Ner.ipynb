{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51d0732c",
      "metadata": {
        "id": "51d0732c"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install transformers"
      ],
      "metadata": {
        "id": "8LGkX_t95Nse"
      },
      "id": "8LGkX_t95Nse",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140b7afb",
      "metadata": {
        "id": "140b7afb"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.tokens import Doc , Span\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.TPUStrategy(tpu)"
      ],
      "metadata": {
        "id": "BKJxOy534iy3"
      },
      "id": "BKJxOy534iy3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec967d98",
      "metadata": {
        "id": "ec967d98"
      },
      "outputs": [],
      "source": [
        "def read_dataset(file_path):\n",
        "    examples = []\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        example = []\n",
        "        for line in file:\n",
        "            line = line.strip()\n",
        "            if line.startswith('-'):\n",
        "                if example:\n",
        "                    examples.append(' '.join(example))\n",
        "                example = [line]\n",
        "            else:\n",
        "                example.append(line)\n",
        "        if example:\n",
        "            examples.append(' '.join(example))\n",
        "    return examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "951d5d2c",
      "metadata": {
        "id": "951d5d2c"
      },
      "outputs": [],
      "source": [
        "def extract_entities(text):\n",
        "    entity_regex = r'\\[(.*?)\\]\\((.*?)\\)'\n",
        "    entities = re.findall(entity_regex, text)\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9581e61f",
      "metadata": {
        "id": "9581e61f"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(data):\n",
        "    training_data = []\n",
        "    for line in data:\n",
        "        entities = extract_entities(line)\n",
        "        doc_text = line\n",
        "        ent_list = []\n",
        "        for entity, tag in entities:\n",
        "            start = line.index(\"[\" + entity + \"]\")\n",
        "            end = start + len(entity)\n",
        "            ent_list.append((start, end, tag))\n",
        "\n",
        "        training_data.append((doc_text, {\"entities\": ent_list}))\n",
        "    return training_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7ad5cec",
      "metadata": {
        "id": "c7ad5cec"
      },
      "outputs": [],
      "source": [
        "file_path = 'labeled.txt'\n",
        "examples = read_dataset(file_path)\n",
        "training_data = preprocess_data(examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57deae1c",
      "metadata": {
        "id": "57deae1c"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d35ad00",
      "metadata": {
        "id": "2d35ad00"
      },
      "outputs": [],
      "source": [
        "input_texts = [text for text, _ in training_data]\n",
        "\n",
        "output_texts = [f\"{text} [<ENT_START>] {tag} [<ENT_END>]\" for text, tags in training_data for tag in tags]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f9554c7",
      "metadata": {
        "id": "1f9554c7"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fa19129",
      "metadata": {
        "id": "6fa19129"
      },
      "outputs": [],
      "source": [
        "input_encodings = tokenizer(input_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
        "output_encodings = tokenizer(output_texts, return_tensors=\"tf\", padding=True, truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_sequence_length = max(input_encodings[\"input_ids\"].shape[1], output_encodings[\"input_ids\"].shape[1])\n",
        "input_encodings = tokenizer(input_texts, return_tensors=\"tf\", padding=\"max_length\", truncation=True, max_length=max_sequence_length)\n",
        "output_encodings = tokenizer(output_texts, return_tensors=\"tf\", padding=\"max_length\", truncation=True, max_length=max_sequence_length)"
      ],
      "metadata": {
        "id": "7b7hPXNJFhFq"
      },
      "id": "7b7hPXNJFhFq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a2f626",
      "metadata": {
        "id": "03a2f626"
      },
      "outputs": [],
      "source": [
        "output_ids = output_encodings[\"input_ids\"]\n",
        "labels = output_ids.numpy().copy()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#output_texts"
      ],
      "metadata": {
        "id": "nVJJlnku66SH"
      },
      "id": "nVJJlnku66SH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b1c638",
      "metadata": {
        "id": "72b1c638"
      },
      "outputs": [],
      "source": [
        "#labels[labels == tokenizer.pad_token_id] = -100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c16865c",
      "metadata": {
        "id": "8c16865c"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(input_encodings[\"input_ids\"]))\n",
        "val_size = int(0.1 * len(input_encodings[\"input_ids\"]))\n",
        "test_size = int(0.1 * len(input_encodings[\"input_ids\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76488915",
      "metadata": {
        "id": "76488915"
      },
      "outputs": [],
      "source": [
        "train_inputs = {k: v[:train_size] for k, v in input_encodings.items()}\n",
        "train_labels = output_ids[:train_size]\n",
        "\n",
        "val_inputs = {k: v[train_size:train_size + val_size] for k, v in input_encodings.items()}\n",
        "val_labels = output_ids[train_size:train_size + val_size]\n",
        "\n",
        "test_inputs = {k: v[-test_size:] for k, v in input_encodings.items()}\n",
        "test_labels = output_ids[-test_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a288313",
      "metadata": {
        "id": "5a288313"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57f0bec1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57f0bec1",
        "outputId": "688ef29a-c521-4b44-9f5f-ca37db86159e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "with strategy.scope():\n",
        "  model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "  model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81101823",
      "metadata": {
        "id": "81101823"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "batch_size = 64\n",
        "steps_per_epoch = len(train_inputs[\"input_ids\"]) // batch_size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33dc20c4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33dc20c4",
        "outputId": "9c3d49a6-6487-45ff-e1e3-1a6da79e58eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "14/14 [==============================] - 55s 590ms/step - loss: 3.0798 - accuracy: 0.5750 - val_loss: 2.6207 - val_accuracy: 0.6874\n",
            "Epoch 2/20\n",
            "14/14 [==============================] - 3s 245ms/step - loss: 1.2581 - accuracy: 0.8451 - val_loss: 1.0382 - val_accuracy: 0.8770\n",
            "Epoch 3/20\n",
            "14/14 [==============================] - 3s 242ms/step - loss: 0.6731 - accuracy: 0.8994 - val_loss: 0.5584 - val_accuracy: 0.9068\n",
            "Epoch 4/20\n",
            "14/14 [==============================] - 3s 227ms/step - loss: 0.4336 - accuracy: 0.9100 - val_loss: 0.3625 - val_accuracy: 0.9176\n",
            "Epoch 5/20\n",
            "14/14 [==============================] - 3s 221ms/step - loss: 0.3328 - accuracy: 0.9164 - val_loss: 0.3325 - val_accuracy: 0.9180\n",
            "Epoch 6/20\n",
            "14/14 [==============================] - 4s 271ms/step - loss: 0.2893 - accuracy: 0.9206 - val_loss: 0.2662 - val_accuracy: 0.9209\n",
            "Epoch 7/20\n",
            "14/14 [==============================] - 3s 241ms/step - loss: 0.2613 - accuracy: 0.9226 - val_loss: 0.2251 - val_accuracy: 0.9265\n",
            "Epoch 8/20\n",
            "14/14 [==============================] - 3s 236ms/step - loss: 0.2502 - accuracy: 0.9234 - val_loss: 0.2175 - val_accuracy: 0.9277\n",
            "Epoch 9/20\n",
            "14/14 [==============================] - 3s 223ms/step - loss: 0.2398 - accuracy: 0.9249 - val_loss: 0.2058 - val_accuracy: 0.9280\n",
            "Epoch 10/20\n",
            "14/14 [==============================] - 3s 222ms/step - loss: 0.2321 - accuracy: 0.9250 - val_loss: 0.1981 - val_accuracy: 0.9290\n",
            "Epoch 11/20\n",
            "14/14 [==============================] - 3s 224ms/step - loss: 0.2273 - accuracy: 0.9262 - val_loss: 0.1860 - val_accuracy: 0.9302\n",
            "Epoch 12/20\n",
            "14/14 [==============================] - 3s 248ms/step - loss: 0.2223 - accuracy: 0.9264 - val_loss: 0.1857 - val_accuracy: 0.9299\n",
            "Epoch 13/20\n",
            "14/14 [==============================] - 3s 236ms/step - loss: 0.2195 - accuracy: 0.9266 - val_loss: 0.1864 - val_accuracy: 0.9303\n",
            "Epoch 14/20\n",
            "14/14 [==============================] - 3s 226ms/step - loss: 0.2170 - accuracy: 0.9270 - val_loss: 0.1823 - val_accuracy: 0.9303\n",
            "Epoch 15/20\n",
            "14/14 [==============================] - 3s 225ms/step - loss: 0.2128 - accuracy: 0.9278 - val_loss: 0.1823 - val_accuracy: 0.9305\n",
            "Epoch 16/20\n",
            "14/14 [==============================] - 3s 227ms/step - loss: 0.2113 - accuracy: 0.9277 - val_loss: 0.1761 - val_accuracy: 0.9310\n",
            "Epoch 17/20\n",
            "14/14 [==============================] - 3s 247ms/step - loss: 0.2076 - accuracy: 0.9281 - val_loss: 0.1702 - val_accuracy: 0.9318\n",
            "Epoch 18/20\n",
            "14/14 [==============================] - 3s 238ms/step - loss: 0.2092 - accuracy: 0.9277 - val_loss: 0.1707 - val_accuracy: 0.9316\n",
            "Epoch 19/20\n",
            "14/14 [==============================] - 3s 220ms/step - loss: 0.2052 - accuracy: 0.9287 - val_loss: 0.1725 - val_accuracy: 0.9311\n",
            "Epoch 20/20\n",
            "14/14 [==============================] - 3s 228ms/step - loss: 0.2048 - accuracy: 0.9278 - val_loss: 0.1679 - val_accuracy: 0.9325\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x780616dff2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model.fit(\n",
        "    train_inputs, train_labels,\n",
        "    batch_size=batch_size,\n",
        "    epochs=num_epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=(val_inputs, val_labels)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a08dd1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a08dd1",
        "outputId": "fd4245dc-428e-42b5-c2e1-cba7b7546b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 71ms/step - loss: 0.1690 - accuracy: 0.9318\n",
            "Test Loss: 0.16901470720767975\n",
            "Test Accuracy: 0.9317948222160339\n"
          ]
        }
      ],
      "source": [
        "test_loss, test_accuracy = model.evaluate(\n",
        "    test_inputs,\n",
        "    test_labels,\n",
        "    batch_size=batch_size\n",
        ")\n",
        "\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"C:\\\\Users\\\\admin\\\\Desktop\")"
      ],
      "metadata": {
        "id": "ti6b2XwFKGAc"
      },
      "id": "ti6b2XwFKGAc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_text = \" - 1000 manatlıq Albalı plüs kart almaq istəyirəm.\""
      ],
      "metadata": {
        "id": "YYUUgMfWKl1a"
      },
      "id": "YYUUgMfWKl1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input_encoding = tokenizer(input_text, return_tensors=\"tf\", padding=True, truncation=True)"
      ],
      "metadata": {
        "id": "8GGMAZgSO1AM"
      },
      "id": "8GGMAZgSO1AM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predictions = model.predict(input_encoding)\n"
      ],
      "metadata": {
        "id": "pPCVVYD4O3rp"
      },
      "id": "pPCVVYD4O3rp",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}